<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ELK on WeiWeiWesley</title>
    <link>/categories/elk/</link>
    <description>Recent content in ELK on WeiWeiWesley</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Sep 2019 14:12:14 +0800</lastBuildDate><atom:link href="/categories/elk/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Redis ELK</title>
      <link>/posts/2019-09-25-redis-elk/</link>
      <pubDate>Wed, 25 Sep 2019 14:12:14 +0800</pubDate>
      
      <guid>/posts/2019-09-25-redis-elk/</guid>
      <description>&lt;p&gt;結合 reids &amp;amp; ELK 搜集分散式 log。適合想統一異質平台或已微服務化的系統使用。&lt;/p&gt;
&lt;p&gt;僅需將目標 log RPUSH 至指定的 redis key，便可輕鬆地藉由 logstash 取出至 elasticserach 最後由 kibana 呈現。&lt;/p&gt;
&lt;h3 id=&#34;需求&#34;&gt;需求&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;docker-compose&lt;/li&gt;
&lt;li&gt;redis&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;repo&#34;&gt;Repo&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/WeiWeiWesley/ELK&#34;&gt;https://github.com/WeiWeiWesley/ELK&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day30 30 days Review</title>
      <link>/posts/2019-01-14-elk/</link>
      <pubDate>Sat, 14 Jan 2017 14:28:00 +0800</pubDate>
      
      <guid>/posts/2019-01-14-elk/</guid>
      <description>&lt;h2 id=&#34;終於終於&#34;&gt;終於終於&lt;/h2&gt;
&lt;p&gt;終於終於走到這裡了，我先向一路看我拖搞到這的人說先抱歉與謝謝。寫這系列文章主要是剛好正在接觸 E.L.K 這套常見 log solution 時，朋友建議把學習的過程記錄下來，加深自己理解的內容也幫助別人可以有簡單的入門方式。當然我知道自己有很多地方的使用方式還不夠理想，還有很多需要改善的地方，還有很多可以發揮它強大效能的地方沒有補足，但我還是很開心的迎接這最後一天的到來 —— Done is better than perfect. 或許很多人對話此嗤之以鼻但我現在懂它的價值所在，如果真的要做到完美的話這系列文章是不會誕生的，他應該在Day23就結束了(笑)。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day29 Optimization</title>
      <link>/posts/2018-01-13-elk/</link>
      <pubDate>Fri, 13 Jan 2017 20:34:56 +0800</pubDate>
      
      <guid>/posts/2018-01-13-elk/</guid>
      <description>&lt;p&gt;在使用 Elastic Stack 的過程中可藉由調整 logstash 資料過濾的方式、elasticsearch 資料儲存的方式來優化整體效能。個人在使用的過程中有遇到效能上的問題，一開始是資源使用不完全，cup的使用率一直無法接近滿載的情況而導致資料處理緩慢。後來想到一個簡單的方式在不改動環境設定值的情況下增加 logstash 的工作量 —— 直接增加 logstash container，這是唯有我們在 docker container 下才有辦法做到的解法，雖然多虛擬機會犧牲部分資源，但能夠用這種方式使得資源被完全使用對當下的我來說是最重要的，如果你跟我一樣只是想多使用一點資源的話這或許是一個簡單不錯的方式。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;But if you want to do it by the right way. 你需要參考&lt;a href=&#34;https://www.elastic.co/guide/en/logstash/current/logstash-settings-file.html&#34;&gt;官方文件&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day28 Kibana Extended Metric Plugin</title>
      <link>/posts/2017-01-12-elk/</link>
      <pubDate>Thu, 12 Jan 2017 23:42:06 +0800</pubDate>
      
      <guid>/posts/2017-01-12-elk/</guid>
      <description>&lt;p&gt;終於快要結束了，其實寫到這裡已經超過我於本預計想分享的部分了，所以很多東西真的是現學現賣，這次也是分享一個小插件的安裝與使用方式，還希望大家不嫌棄囉。&lt;/p&gt;
&lt;h2 id=&#34;kibana-extended-metric-plugin&#34;&gt;Kibana Extended Metric Plugin&lt;/h2&gt;
&lt;p&gt;這個插件十分簡易，這邊直接提供測試資料與設定檔供大家，下載試玩。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day27 地震熱區資料視覺化測試範例</title>
      <link>/posts/2017-01-11-elk/</link>
      <pubDate>Wed, 11 Jan 2017 23:42:06 +0800</pubDate>
      
      <guid>/posts/2017-01-11-elk/</guid>
      <description>&lt;p&gt;這篇其實使用的技巧與上篇差異不大，主要是提供一個資料，讓大家可以快速做出 Dashboard 並調整成符合自己的需求。當然你也可以參照 logstash.conf 調整一下資料過濾的方式，之前一直沒有提到的 grok 小技巧是其實你可以利用 (?:ValueA | ValueB) 的方式給予 field 預設值。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day26 Apache log 測試範例懶人包</title>
      <link>/posts/2017-01-10-elk/</link>
      <pubDate>Tue, 10 Jan 2017 22:19:06 +0800</pubDate>
      
      <guid>/posts/2017-01-10-elk/</guid>
      <description>&lt;p&gt;提醒沒看過前面的朋友想直接拿首先基本安裝還是要有，可參考 [Day2 - 來個快速安裝吧](- 來個快速安裝吧) 獲得基本環境部署，開啟 &lt;a href=&#34;http://127.0.0.1:5601/app/kibana&#34;&gt;Kibana local&lt;/a&gt; 確定服務能正確啟動。&lt;/p&gt;
&lt;p&gt;來囉快速咻咻咻完成它&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day25 Kibana X-Pack Graph</title>
      <link>/posts/2017-01-09-elk/</link>
      <pubDate>Mon, 09 Jan 2017 23:11:23 +0800</pubDate>
      
      <guid>/posts/2017-01-09-elk/</guid>
      <description>&lt;p&gt;再開始講 Graph 之前我要跟大家提醒一下加裝 X-Pack 後你可能會遇到無法使用 tcp 當 input 的情況，然後一直跑去檢查 logstash container 的設定卻又苦無結果，其實這不是 logstash 壞掉哦!是 elasticsearch 上的 x-pack 因增加了權限管理而把 logstash 送過來的請求擋掉了，我們僅需簡單地在送至 elasticsearch 時帶上帳號密碼即可。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day24 Kinban Plugin X-Pack Monitoring</title>
      <link>/posts/2017-01-08-elk/</link>
      <pubDate>Sun, 08 Jan 2017 22:05:35 +0800</pubDate>
      
      <guid>/posts/2017-01-08-elk/</guid>
      <description>&lt;p&gt;不好意思昨天出了意料之外的錯誤 (註：其實這篇應該昨天就要打出來的)，其實我也是一邊查資料一邊安裝 x-pack 這個 kibana plugin &lt;img src=&#34;/ithome/emoticon16.gif&#34; alt=&#34;emoticon16.gif&#34;&gt;&lt;/p&gt;
&lt;p&gt;所以先前雖然裝好了 x-pack 也成功登入我們的 kibana web 介面，但最重要的就是這個 &amp;ldquo;but&amp;rdquo;，他完全沒有達到我本來想跟大家分享的功能，觀察 elasticsearch cluster nodes。其實僅是我在安裝的時候竟然！竟然！只在 elasticsearch 一個節點上未安裝 x-pack&lt;/p&gt;
&lt;p&gt;這下真是尷尬今天會再重新分享一下更簡單的安裝方式，以及 X-Pack 提供的主要功能。想用昨天那種直接進入容器的方式安裝也可以，只是要記得所有的 elasticsearch nodes 都要裝哦～要是你覺得這樣太麻煩了這邊提供一個比較優雅的方式，也是一樣直接在啟動容器上改變設置：&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day23 Kibana Plugin X-Pack Install</title>
      <link>/posts/2017-01-07-elk/</link>
      <pubDate>Sat, 07 Jan 2017 23:37:38 +0800</pubDate>
      
      <guid>/posts/2017-01-07-elk/</guid>
      <description>&lt;p&gt;今天要介紹一個超好用的 kibana plugin，X-Pack。你一定要安裝的好用插件讓我們先裝在說。&lt;/p&gt;
&lt;p&gt;就讓我們照著&lt;a href=&#34;https://www.elastic.co/downloads/x-pack&#34;&gt;官網&lt;/a&gt;的六步驟來安裝吧(只是我們是裝在 docker 上)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day22 Multi logstash machines used for monitoring.</title>
      <link>/posts/2017-01-06-elk/</link>
      <pubDate>Fri, 06 Jan 2017 17:30:01 +0800</pubDate>
      
      <guid>/posts/2017-01-06-elk/</guid>
      <description>&lt;p&gt;今天介紹的是一個利用虛擬機使用 logstash 的技巧，當我們有一台較高效能實體機器且有多個不同的資料來源時，我們可能會把 logstash.conf 用 file 檔名或 tcp port 來進行區別如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;input&lt;/span&gt; {
        &lt;span style=&#34;color:#a6e22e&#34;&gt;tcp&lt;/span&gt; {
                &lt;span style=&#34;color:#a6e22e&#34;&gt;port&lt;/span&gt; =&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5000&lt;/span&gt;
                &lt;span style=&#34;color:#a6e22e&#34;&gt;type&lt;/span&gt; =&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;access&amp;#34;&lt;/span&gt;
        }

        &lt;span style=&#34;color:#a6e22e&#34;&gt;tcp&lt;/span&gt; {
                &lt;span style=&#34;color:#a6e22e&#34;&gt;port&lt;/span&gt; =&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;5001&lt;/span&gt;
                &lt;span style=&#34;color:#a6e22e&#34;&gt;type&lt;/span&gt; =&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;error&amp;#34;&lt;/span&gt;
        }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day21 Elasticsearch Cluster</title>
      <link>/posts/2017-01-05-elk/</link>
      <pubDate>Thu, 05 Jan 2017 23:49:30 +0800</pubDate>
      
      <guid>/posts/2017-01-05-elk/</guid>
      <description>&lt;p&gt;我們進行到今天已經可以將 E.L.K 運行並實現資料分析、資料儲存、資料探索與視覺化，現在我們需要做的是提升它的可靠性與面對效能瓶頸時拓展自身的能力。這時候 elasticsearch cluster 就派上用場拉，至少要讓你的資料能夠有個備份，假設 docker container 毀損甚至機器毀損時能不受引響。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day20 Dockerfile &amp; docker-compose</title>
      <link>/posts/2017-01-04-elk/</link>
      <pubDate>Wed, 04 Jan 2017 20:50:41 +0800</pubDate>
      
      <guid>/posts/2017-01-04-elk/</guid>
      <description>&lt;p&gt;有了 docker 基本的概念後我們回頭看一下 docker 的設定檔 Dockerfile 與 docker-compose.yml，瞭解一下 logstash, elasticsearch, kibana 的虛擬機是怎麼被設定與啟用的。有了這些知識後能幫助我們解決一些 E.L.K 使用上或配置上的問題。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day19 Docker</title>
      <link>/posts/2017-01-03-elk/</link>
      <pubDate>Tue, 03 Jan 2017 19:47:23 +0800</pubDate>
      
      <guid>/posts/2017-01-03-elk/</guid>
      <description>&lt;p&gt;你以為看錯標題了嗎？沒有哦～就是 docker 沒錯，為了延續後面我要講一些插件與優化配置的方式，這邊必須簡單提一下 docker 讓後面在能夠比較簡單的銜接上，真的是簡單講一下因為接觸 docker 時間不夠長，讓我們快速走過吧。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day18 Dashboard</title>
      <link>/posts/2017-01-02-elk/</link>
      <pubDate>Mon, 02 Jan 2017 20:12:59 +0800</pubDate>
      
      <guid>/posts/2017-01-02-elk/</guid>
      <description>&lt;p&gt;先前我們在 Visualize 繪製了許多圖表，這些圖表可能個別代表不同資訊內容或比較，當然我們有可能需要整合這些資訊到同一個頁面裡，這時侯 Dashboard 就登場拉～多個願望一次滿足。當然前提是你要在 Visualize 繪製儲存所需的圖表然後到 Dashboard 就可以直接加入囉。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day17 Bar chart. Tag Cloud. Tile map</title>
      <link>/posts/2017-01-01-elk/</link>
      <pubDate>Sun, 01 Jan 2017 23:34:12 +0800</pubDate>
      
      <guid>/posts/2017-01-01-elk/</guid>
      <description>&lt;p&gt;我們今天來把 kiban visualize 的部分收個尾，首先介紹的是最常用到的 bar chart，設置的方式與 line chart, area chart 相同都是現設定好 x軸、y軸，再加入想要聚合的屬性。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day16 Visualize data with pie chart. Count by metric</title>
      <link>/posts/2016-12-31-elk/</link>
      <pubDate>Sat, 31 Dec 2016 19:00:27 +0800</pubDate>
      
      <guid>/posts/2016-12-31-elk/</guid>
      <description>&lt;p&gt;另個常見的視覺化方式“圓餅圖 pie chart”，圓餅圖可以很有效地對資料進行展示。特別是在想要表示某個大扇型區在整體中所占比例，這一方法十分有效。設置流程與其他圖表大同小異，讓我們快速的一步驟一步驟完成他吧。若用範例資料進行練習，資料切割配置可參考Day15於此不重複說明囉。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day15 DataTable</title>
      <link>/posts/2016-12-30-elk/</link>
      <pubDate>Fri, 30 Dec 2016 18:23:53 +0800</pubDate>
      
      <guid>/posts/2016-12-30-elk/</guid>
      <description>&lt;p&gt;好了我來還債了，我們繼續來看看 kibana visualize 還有什麼可以提供的。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day14 Visualize data with area chart. Markdown your document.</title>
      <link>/posts/2016-12-29-elk/</link>
      <pubDate>Thu, 29 Dec 2016 22:42:13 +0800</pubDate>
      
      <guid>/posts/2016-12-29-elk/</guid>
      <description>&lt;p&gt;Area chart 基本繪製與 line chart 無異&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day13 Visualize data with line chart &amp; bubble chart</title>
      <link>/posts/2016-12-28-elk/</link>
      <pubDate>Wed, 28 Dec 2016 21:29:57 +0800</pubDate>
      
      <guid>/posts/2016-12-28-elk/</guid>
      <description>&lt;p&gt;今天要來畫畫圖兒，如果想完全照著做的話請使用下方 logstash.conf 檔配置，資料來源一樣使用&lt;a href=&#34;http://www.monitorware.com/en/logsamples/apache.php&#34;&gt;apache sample logs&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day12 Reformate timestamp</title>
      <link>/posts/2016-12-27-elk/</link>
      <pubDate>Tue, 27 Dec 2016 21:50:44 +0800</pubDate>
      
      <guid>/posts/2016-12-27-elk/</guid>
      <description>&lt;p&gt;延續先前 Discover 的部分，如何讓我們的紀錄時間 match 上 log 的紀錄時間呢?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day11 Discover data freely by any shape anytime</title>
      <link>/posts/2016-12-26-elk/</link>
      <pubDate>Mon, 26 Dec 2016 13:50:19 +0800</pubDate>
      
      <guid>/posts/2016-12-26-elk/</guid>
      <description>&lt;p&gt;首先我們先抓一份測試用的 apache log 協助我們有相同的資料可以做解說。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day10 Kibana</title>
      <link>/posts/2016-12-25-elk/</link>
      <pubDate>Sun, 25 Dec 2016 21:09:52 +0800</pubDate>
      
      <guid>/posts/2016-12-25-elk/</guid>
      <description>&lt;p&gt;Ola~讓我們開心的使用圖形化介面吧。
終於講到可以讓我們把資料以 html 顯示的方式囉，kibana 也就是 E.L.K 裡的 Ｋ將由 logstash 過濾完並儲存到 elasticsearch 的資料透過設定 index 進行索引。今天就先讓我們來檢視一下介面上有哪些區塊及其所屬的功能。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day9 Elasticsearch 狀態檢視</title>
      <link>/posts/2016-12-24-elk/</link>
      <pubDate>Sat, 24 Dec 2016 21:43:22 +0800</pubDate>
      
      <guid>/posts/2016-12-24-elk/</guid>
      <description>&lt;p&gt;今天簡單介紹一下幾個觀察 elasticsearch 的指令與方法，讓我從個人最常用的index目錄說起。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day8 Elasticsearch CRUD</title>
      <link>/posts/2016-12-23-elk/</link>
      <pubDate>Fri, 23 Dec 2016 23:10:12 +0800</pubDate>
      
      <guid>/posts/2016-12-23-elk/</guid>
      <description>&lt;p&gt;之前提過 elasticsearh 是個擁有全 RESTful API 操作的分散式資料庫，於且向大家介紹一下其基本操作的方法。雖然不見得在使用 E.L.K 會用到，畢竟解析由 logstash 完成了，一般存取也可以由 kibana 完成，但如果想針對單筆紀錄作微調或測試你還是會用到以 curl CRUD 操作的。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day7 Elasticsearch</title>
      <link>/posts/2016-12-22-elk/</link>
      <pubDate>Thu, 22 Dec 2016 19:50:02 +0800</pubDate>
      
      <guid>/posts/2016-12-22-elk/</guid>
      <description>&lt;p&gt;Elasticsearch 是我們之後再 kibana 之所以可以快速的呼叫與搜尋的關鍵，我們來簡單暸解一下 elasticsearch 的特性與他在 E.L.K 中扮演的角色。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day6 便利的 logstsh plugin (filter - 2)</title>
      <link>/posts/2016-12-21-elk/</link>
      <pubDate>Wed, 21 Dec 2016 14:12:52 +0800</pubDate>
      
      <guid>/posts/2016-12-21-elk/</guid>
      <description>&lt;p&gt;今天繼續來看一下有什麼實用的 filter plugins 可以協助我們把資訊從原始資料切割出來。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day5  便利的 logstash plugin (filter)</title>
      <link>/posts/2016-12-20-elk/</link>
      <pubDate>Tue, 20 Dec 2016 20:21:00 +0800</pubDate>
      
      <guid>/posts/2016-12-20-elk/</guid>
      <description>&lt;p&gt;Filter plugins 可以說是整個 logstash 最核心的部分，多數你想要過濾的資料內容都可以仰賴其進行。而 filter plugins 的選擇與配置多數相關於一開始輸入的資料類型，舉例來說你的資料可能含有 geographical location 那你的 filter plugins 就可以加上 geoip ，來幫助你切割出經緯度、地區、國家、時區等資訊。以下簡單介紹一下幾個實用的 filter plugins 使用方式。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day4  便利的 logstash plugin (input)</title>
      <link>/posts/2016-12-19-elk/</link>
      <pubDate>Mon, 19 Dec 2016 17:33:23 +0800</pubDate>
      
      <guid>/posts/2016-12-19-elk/</guid>
      <description>&lt;p&gt;我們在昨天的內容分享了基本的 logstash 使用方式，今天要介紹的則是 logstash 的 input plugins 。其實 logstash 的運作除了基於其 input, filter, ouput 的架構外，在這三層架構中使用的 plugins 才是讓整個 logstash 具有資料切割與過濾能力的核心關鍵，input plugins 提供了我們獲取各式不同資料來源的方式，以下分別介紹幾個常用的 input plugin。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day3  logstash 輸入配置</title>
      <link>/posts/2016-12-18-elk/</link>
      <pubDate>Sun, 18 Dec 2016 19:52:07 +0800</pubDate>
      
      <guid>/posts/2016-12-18-elk/</guid>
      <description>&lt;p&gt;首先 logstash 是 E.L.K 最先觸碰到你想要管理 log 部分，你可以把他想成一個介接層也是首次接觸到 log 或資料的地方，在這裡你可以選擇你的輸入來源像是直接讀 log 檔，抑或是當你想直接接受多台機器的 log 時你可以使用 tcp 作為接口。其次當多個來源的資料想分別處理成不同資訊時，在 logstash 提供的 input 可以協助我們預先做出簡單的類別判斷，以利後續 filter 分別使用不同的規則進行切割，最後於 output 選擇要輸出的方式與位置。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day2 來個快速安裝吧</title>
      <link>/posts/2016-12-17-elk/</link>
      <pubDate>Sat, 17 Dec 2016 14:52:07 +0800</pubDate>
      
      <guid>/posts/2016-12-17-elk/</guid>
      <description>&lt;h2 id=&#34;安裝部屬-&#34;&gt;安裝部屬 :&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;安裝 docker
&lt;a href=&#34;https://joshhu.gitbooks.io/docker_theory_install/content/DockerBible/ubuntuvm.html&#34;&gt;https://joshhu.gitbooks.io/docker_theory_install/content/DockerBible/ubuntuvm.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>30Days ELK - Day1 Why E.L.K</title>
      <link>/posts/2016-12-16-elk/</link>
      <pubDate>Fri, 16 Dec 2016 14:52:07 +0800</pubDate>
      
      <guid>/posts/2016-12-16-elk/</guid>
      <description>&lt;p&gt;初次接觸 E.L.K 希望藉由記錄下學習過程，幫助自己與初次接觸 E.L.K的新手了解其安裝的過程、使用的方法與可能會遇到的困難。首先我們先了解一下 E.L.K 是什麼?&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
